{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Haroon Wajid 21i-1763 NLP-A"
      ],
      "metadata": {
        "id": "_m7qXbkFKltH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11_QtarKSjPf"
      },
      "source": [
        "### Natural Language Processing: Assignment 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S_p54xNSTq_"
      },
      "source": [
        "##### This is demo program which give you idea about how to start  your assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIhS278lWQOv"
      },
      "outputs": [],
      "source": [
        "## for installing UrduHack\n",
        "pip install urduhack[tf]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XChiPrRsSwjz"
      },
      "source": [
        "##### Reading File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxjaBCgOWYKt",
        "outputId": "afd52927-fd82-40f5-dcad-2fb5e79fd063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو۔\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "with open('sent-test.txt', 'rt', encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    passage = list(reader)\n",
        "f.close()\n",
        "text = passage[0][0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qde0a3I6G3rc"
      },
      "source": [
        "##### Segmentation using UrduHack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W65iJImpG3rc",
        "outputId": "d3c1c993-479c-48fa-98ac-eff75ae05cb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے۔',\n",
              " 'کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو۔']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urduhack\n",
        "from urduhack.tokenization import sentence_tokenizer\n",
        "\n",
        "sentences = sentence_tokenizer(text)\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcTi1UuIG3rd",
        "outputId": "98a685a4-cb95-4883-9580-fe1b875b4281"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gaElHuGS4Qb"
      },
      "source": [
        "#### Reading Segmented File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnPi90zhegAM"
      },
      "outputs": [],
      "source": [
        "with open('sent-segmented.txt', 'rt', encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    segmented = list(reader)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_fuq83DG3rd",
        "outputId": "fd9263ed-cf08-490f-ed97-fd2260d4f423"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seg_text = segmented[0][0]\n",
        "sents = seg_text.split('۔')\n",
        "len(sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb9uq7aOG3re"
      },
      "outputs": [],
      "source": [
        "w_list = word_tokenizer(seg_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA_1p0IbG3re",
        "outputId": "a4b154a8-0697-4909-e685-fc95ed26e6eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['عقل',\n",
              " 'خان',\n",
              " 'کے',\n",
              " 'مطابق',\n",
              " 'اس',\n",
              " 'خوبصورت',\n",
              " 'چراگاہ',\n",
              " 'کو',\n",
              " 'کنڈیل',\n",
              " 'شئیبانال',\n",
              " 'کہا',\n",
              " 'جاتا',\n",
              " 'ہے۔',\n",
              " 'کنڈیل',\n",
              " 'شئی',\n",
              " 'بانال',\n",
              " 'کے',\n",
              " 'اس',\n",
              " 'خوبصورت',\n",
              " 'میدان',\n",
              " 'کو',\n",
              " 'اگر',\n",
              " 'سویٹزرلینڈ',\n",
              " 'کے',\n",
              " 'کسی',\n",
              " 'ہرے',\n",
              " 'بھرے',\n",
              " 'میدانی',\n",
              " 'علاقے',\n",
              " 'سے',\n",
              " 'تشبیہہ',\n",
              " 'دی',\n",
              " 'جائے',\n",
              " 'تو',\n",
              " 'کچھ',\n",
              " 'غلط',\n",
              " 'نہیں',\n",
              " 'ہو',\n",
              " 'گا۔میدان',\n",
              " 'میں',\n",
              " 'داخل',\n",
              " 'ہوتے',\n",
              " 'ہی',\n",
              " 'کچھ',\n",
              " 'دیر',\n",
              " 'آرام',\n",
              " 'کرنے',\n",
              " 'کی',\n",
              " 'میری',\n",
              " 'خواہش',\n",
              " 'پر',\n",
              " 'سب',\n",
              " 'نے',\n",
              " 'لبیک',\n",
              " 'کہا۔',\n",
              " 'ایسا',\n",
              " 'لگا',\n",
              " 'جیسے',\n",
              " 'ان',\n",
              " 'کی',\n",
              " 'دل',\n",
              " 'کی',\n",
              " 'بات',\n",
              " 'میرے',\n",
              " 'لبوں',\n",
              " 'سے',\n",
              " 'ادا',\n",
              " 'ہوئی',\n",
              " 'ہو۔']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew26niQIG3re"
      },
      "source": [
        "# Your Sentence Segmentation Code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script segments Urdu text into sentences using a combination of punctuation rules and common sentence-ending phrases. It normalizes the text by removing extra spaces and ensuring proper spacing around punctuation marks like \"۔\" and \"؟\". The core of the script involves a predefined list of Urdu phrases (e.g., \"رہا ہے\", \"ہو گیا\") that typically mark the end of a sentence. The text is first split based on punctuation, and then further refined by checking for these phrases to avoid incorrect splits. Finally, the segmented sentences are saved into a CSV file for easy access and further processing. This approach ensures accurate segmentation of Urdu text, especially in complex sentence structures, by maintaining the integrity of common sentence-ending expressions."
      ],
      "metadata": {
        "id": "tgm0OGpyJQgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU7hKxl0G3rf",
        "outputId": "9e651c33-c772-462c-860d-06846eb3a963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation complete. Sentences saved to 'segmented_sentences.csv'.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Updated list of end-of-sentence words in pairs\n",
        "end_of_sentence_pairs = [\n",
        "    'رہا ہے', 'ہوا ہے', 'رہی ہے', 'ہو چکا', 'ہو چکی', 'ہو گئے', 'ہو گیا',\n",
        "    'کر چکا', 'کر چکی', 'کر دیا', 'کر دی', 'کر رہا', 'کر رہی', 'ختم ہو گیا',\n",
        "    'آ چکا', 'گئی ہے', 'آ چکی', 'چلا گیا', 'چلی گئی', 'بڑھ گیا', 'بن چکا',\n",
        "    'بن گئی', 'بن چکی', 'ختم ہو چکی', 'رکھا ہے', 'پہنچ گیا', 'لگا دیا',\n",
        "    'دیا گیا', 'دیا گیا ہے', 'دے دیا', 'بنا دیا', 'لگا دی', 'کہا ہے',\n",
        "    'دیکھا گیا','جاتا ہے','ہیں','تھا', 'تھی','دیا','رہے ہیں','آئے',\n",
        "    'آئے ہیں', 'سمجھا ہے', 'جانا ہے', 'چکنا ہے', 'لیا گیا', 'کہہ دیا',\n",
        "    'پڑھ گیا', 'کہی گئی', 'سنا گیا', 'کھڑا ہے', 'سکتا ہے', 'پکڑ لیا',\n",
        "    'سنا ہے', 'جانے لگا', 'جانے لگی', 'چلتا رہا', 'کھا لیا', 'ڈالا گیا',\n",
        "    'ڈالی گئی', 'گزر گیا', 'گزر چکی', 'ہو رہا ہے', 'رکھ دیا', 'کیا گیا',\n",
        "    'کیا گیا ہے', 'چھوڑ دیا', 'نکال دیا', 'بٹھا دیا', 'بیٹھ گیا', 'آنے لگا',\n",
        "    'بچا لیا', 'گرا دیا', 'کھڑا ہو گیا', 'ڈھونڈ لیا', 'دکھا دیا', 'چھوڑ دیا ہے',\n",
        "    'سن لیا', 'چمک رہا', 'بن گیا', 'باندھ دیا', 'کھول دیا', 'کھا گیا',\n",
        "    'سنائی دی', 'گزر چکا', 'ٹوٹ گیا', 'بچ گیا', 'سنائی گئی', 'مان لیا',\n",
        "    'پڑھ چکا', 'سمجھ گیا', 'گزار دی', 'ٹوٹ چکی', 'پہنچا دیا', 'لے لیا',\n",
        "    'بول چکا', 'ہنس دیا', 'آیا تھا', 'بول دیا', 'چل چکا', 'جا چکا','، دیتا'\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Function to normalize Urdu text (handling spaces, punctuation)\n",
        "def normalize_urdu_text(text):\n",
        "    # Remove extra spaces between words\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Normalize punctuation by ensuring spaces are around it\n",
        "    text = re.sub(r'(?<=[۔!?])\\s*', ' ', text)  # Remove space after sentence-ending punctuation\n",
        "    text = re.sub(r'(?<=\\w)[،؛]', ' ', text)  # Ensure space before certain punctuation\n",
        "    return text.strip()\n",
        "\n",
        "# Function to segment sentences based on punctuation and paired end-of-sentence words\n",
        "def segment_sentences(text):\n",
        "    normalized_text = normalize_urdu_text(text)\n",
        "\n",
        "    # Split text on sentence-ending punctuation\n",
        "    sentences = re.split(r'(?<=[۔!?])\\s*', normalized_text)\n",
        "\n",
        "    # Further refine the segmentation based on end-of-sentence pairs\n",
        "    segmented_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        i = 0\n",
        "        while i < len(words):\n",
        "            # Join pairs like \"رہا ہے\" together to avoid segmentation issues\n",
        "            for pair in end_of_sentence_pairs:\n",
        "                pair_words = pair.split()\n",
        "                if ' '.join(words[i:i+2]) == pair:\n",
        "                    # Add the entire sentence until the end-of-sentence pair\n",
        "                    segmented_sentences.append(' '.join(words[:i+2]))\n",
        "                    words = words[i+2:]  # Remove these words from the list\n",
        "                    i = 0\n",
        "                    break\n",
        "            else:\n",
        "                i += 1  # Move to the next word\n",
        "        if words:\n",
        "            segmented_sentences.append(' '.join(words))\n",
        "\n",
        "    return [s.strip() for s in segmented_sentences if s]\n",
        "\n",
        "# Load the 'urdu-corpus.txt' file and segment it\n",
        "with open('urdu-corpus.txt', 'r', encoding='utf-8') as file:\n",
        "    urdu_text = file.read()\n",
        "\n",
        "# Normalize and segment the corpus\n",
        "segmented_sentences = segment_sentences(urdu_text)\n",
        "\n",
        "# Save the segmented sentences to a CSV file\n",
        "import csv\n",
        "\n",
        "with open('segmented_sentences.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    for sentence in segmented_sentences:\n",
        "        writer.writerow([sentence])\n",
        "\n",
        "print(\"Segmentation complete. Sentences saved to 'segmented_sentences.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ83jAJ3S-uR"
      },
      "source": [
        "# Compute Accuracy and Print Results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates the accuracy of a sentence segmentation process by comparing the total number of segmented sentences to an expected count. It defines an expected_sentence_count of 30,000 and then counts the actual total_segmented_count from the segmented sentences. Accuracy is computed as the percentage of the segmented sentences relative to the expected count."
      ],
      "metadata": {
        "id": "nJpt6PIaJf0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyFZPRIGlWNy",
        "outputId": "2dc14ca8-d489-496e-88f8-52300b1f91ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Segmented Sentences: 17238\n",
            "Expected Sentence Count: 30000\n",
            "Accuracy: 57.46%\n"
          ]
        }
      ],
      "source": [
        "# Define the expected number of sentences\n",
        "expected_sentence_count = 30000\n",
        "# Count the total number of segmented sentences generated\n",
        "total_segmented_count = len(segmented_sentences)\n",
        "\n",
        "# Calculate accuracy based on the number of segmented sentences\n",
        "if expected_sentence_count > 0:\n",
        "    accuracy = (total_segmented_count / expected_sentence_count) * 100\n",
        "else:\n",
        "    accuracy = 0\n",
        "\n",
        "print(f\"Total Segmented Sentences: {total_segmented_count}\")\n",
        "print(f\"Expected Sentence Count: {expected_sentence_count}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aewlt1mNG3rf"
      },
      "source": [
        "# Your SentencePiece Code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process begins with the get_vocab(text) function, which converts each word in the input text into a sequence of characters, appending an end-of-word marker </w>. It then counts the frequency of these word forms.\n",
        "\n",
        "Next, the get_stats(vocab) function identifies and counts the frequency of adjacent symbol pairs (characters or subwords) across all words in the vocabulary.\n",
        "\n",
        "The merge_vocab(pair, v_in) function then merges the most frequent symbol pair into a single unit, updating the vocabulary by replacing the pair with the merged symbol in all relevant words.\n",
        "\n",
        "This merging process is repeated iteratively, gradually reducing the vocabulary size by combining common subword units, which leads to an efficient encoding of text with a compact vocabulary.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JPsGkwY7J1-S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "acn4uPVxG3rf"
      },
      "outputs": [],
      "source": [
        "# Function to perform BPE (Byte Pair Encoding)\n",
        "from collections import Counter\n",
        "def get_vocab(text):\n",
        "    words = [' '.join(word) + ' </w>' for word in text.split()]\n",
        "    vocab = Counter(words)\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    for word in v_in:\n",
        "        w_out = word.replace(bigram, ''.join(pair))\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOaBn_xDG3rf"
      },
      "source": [
        "# SentencePiece encode() and decode() functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SentencePiece Encoding:\n",
        "sentencepiece_encode: This function encodes a sentence by iterating through each character, progressively forming words. It checks if the current word matches any token from a precomputed list of subword tokens (generated through BPE). If a match is found, the token is added to the list of tokens, and the matched part is removed from the word. Spaces and punctuation are treated as individual tokens.\n",
        "## SentencePiece Decoding:\n",
        "sentencepiece_decode: This function decodes a list of tokens back into a sentence by concatenating them. It adds spaces where necessary, especially around punctuation marks, to ensure the sentence is properly formatted.\n",
        "### Workflow:\n",
        "**Normalization and Segmentation:** The text is normalized and segmented into sentences.\n",
        "**Vocabulary Generation with BPE:** The vocabulary is generated by iteratively merging the most frequent symbol pairs (characters or subwords) using BPE over 1000 iterations.\n",
        "**Encoding and Decoding:** Each segmented sentence is encoded using SentencePiece, and then decoded back to verify the process.\n",
        "This approach helps in effectively tokenizing and processing text, especially for languages like Urdu, ensuring that common subwords are represented compactly and sentences are correctly encoded and decoded."
      ],
      "metadata": {
        "id": "zbNYDRUbKNmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSJY-vSOG3rg",
        "outputId": "1fe95f8b-03a8-433e-b44a-5c497ed1c86e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Sentence 1: بے چاری عوام چونکہ ہمیشہ سے دھوکہ کھانے کی عادی رہی ہے\n",
            "Segmented Sentence 2: اس لئے تبدیلی سرکار کی چکنی چپڑی باتوں میں آگئی اور اپنے بہتر مستقبل کے لئے نئی حکومت کو اقتدار کے ایوانوں تک پہنچا دیا\n",
            "Encoded Sentence 1: ['بے ', 'چاری ', 'عوام ', 'چونکہ ', 'ہمیشہ ', 'سے ', 'دھوکہ ', 'کھانے ', 'کی ', 'عادی ', 'رہی ']\n",
            "Decoded Sentence 1: بے چاری عوام چونکہ ہمیشہ سے دھوکہ کھانے کی عادی رہی\n",
            "Encoded Sentence 2: ['اس ', 'لئے ', 'تبدیلی ', 'سرکار ', 'کی ', 'چکنی ', 'چپڑی ', 'باتوں ', 'میں ', 'آگئی ', 'اور ', 'اپنے ', 'بہتر ', 'مستقبل ', 'کے ', 'لئے ', 'نئی ', 'حکومت ', 'کو ', 'اقتدار ', 'کے ', 'ایوانوں ', 'تک ', 'پہنچا ']\n",
            "Decoded Sentence 2: اس لئے تبدیلی سرکار کی چکنی چپڑی باتوں میں آگئی اور اپنے بہتر مستقبل کے لئے نئی حکومت کو اقتدار کے ایوانوں تک پہنچا\n"
          ]
        }
      ],
      "source": [
        "def sentencepiece_encode(sentence, subword_tokens):\n",
        "    tokens = []\n",
        "    word = ''\n",
        "    for char in sentence:\n",
        "        word += char\n",
        "        # Check if the current word can be tokenized\n",
        "        for token in sorted(subword_tokens, key=len, reverse=True):\n",
        "            if word.startswith(token):\n",
        "                tokens.append(token)\n",
        "                word = word[len(token):]  # Remove token part from the word\n",
        "                break\n",
        "        # Add space or punctuation as individual tokens\n",
        "        if word and re.search(r'\\s|[،؛.!?]', word):\n",
        "            tokens.append(word)\n",
        "            word = ''\n",
        "    return tokens\n",
        "\n",
        "def sentencepiece_decode(tokens):\n",
        "    sentence = ''\n",
        "    for token in tokens:\n",
        "        if token in ['</w>']:\n",
        "            sentence += ' '\n",
        "        else:\n",
        "            sentence += token\n",
        "    # Post-process to ensure space around punctuation\n",
        "    sentence = re.sub(r'(\\w)([،؛.!?])', r'\\1 \\2', sentence)\n",
        "    return sentence.strip()\n",
        "\n",
        "# Example sentence for segmentation\n",
        "example_sentence = (\n",
        "    \"بے چاری عوام چونکہ ہمیشہ سے دھوکہ کھانے کی عادی رہی ہے اس لئے \"\n",
        "    \"تبدیلی سرکار کی چکنی چپڑی باتوں میں آگئی اور اپنے بہتر مستقبل \"\n",
        "    \"کے لئے نئی حکومت کو اقتدار کے ایوانوں تک پہنچا دیا\"\n",
        ")\n",
        "\n",
        "# Normalize the text\n",
        "normalized_text = normalize_urdu_text(example_sentence)\n",
        "\n",
        "# Segment the text into sentences\n",
        "segmented_sentences = segment_sentences(normalized_text)\n",
        "\n",
        "# Display the segmented sentences\n",
        "for idx, sentence in enumerate(segmented_sentences, start=1):\n",
        "    print(f'Segmented Sentence {idx}: {sentence}')\n",
        "\n",
        "# Generate the vocabulary\n",
        "vocab = get_vocab(normalized_text)\n",
        "\n",
        "# Perform BPE for a certain number of iterations to merge most common pairs\n",
        "num_merges = 1000\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "\n",
        "# Extract the final subword tokens (vocabulary)\n",
        "subword_tokens = list(set(' '.join(vocab).split()))\n",
        "\n",
        "# Example encoding and decoding of segmented sentences\n",
        "for idx, sentence in enumerate(segmented_sentences, start=1):\n",
        "    encoded_sentence = sentencepiece_encode(sentence, subword_tokens)\n",
        "    decoded_sentence = sentencepiece_decode(encoded_sentence)\n",
        "\n",
        "    print(f'Encoded Sentence {idx}: {encoded_sentence}')\n",
        "    print(f'Decoded Sentence {idx}: {decoded_sentence}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}